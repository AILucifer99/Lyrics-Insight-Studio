{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22d1ce7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages found :- 15\n",
      "\n",
      "Total splitted documents chunks created are :- 92\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader, PyPDFLoader\n",
    "from langchain_google_genai import (\n",
    "    GoogleGenerativeAIEmbeddings, \n",
    "    ChatGoogleGenerativeAI\n",
    ")\n",
    "from langchain_openai import (\n",
    "    ChatOpenAI, \n",
    "    OpenAIEmbeddings\n",
    ")\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(\n",
    "    find_dotenv()\n",
    ")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "data_path = \"Data\\\\Attention.pdf\"\n",
    "documents = PyPDFLoader(file_path=data_path).load()\n",
    "print(\"Total pages found :- {}\\n\".format(len(documents)))\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512, \n",
    "    chunk_overlap=64,\n",
    "    length_function=len,\n",
    "    is_separator_regex=True,\n",
    ")\n",
    "\n",
    "texts = splitter.split_documents(documents)\n",
    "print(\"Total splitted documents chunks created are :- {}\\n\".format(len(texts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d5537cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, text in enumerate(texts) :\n",
    "    text.metadata[\"id\"] = index\n",
    "\n",
    "\n",
    "embeddings_google = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\"\n",
    ")\n",
    "embeddings_openai = OpenAIEmbeddings()\n",
    "\n",
    "\n",
    "llm_model_google = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-001\", \n",
    "    temperature=0.4, \n",
    "    max_tokens=1024, \n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "llm_model_openai = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.4, \n",
    "    max_tokens=1024, \n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "retriever = FAISS.from_documents(\n",
    "    texts, embeddings_google,\n",
    ").as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\" : 10}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fde6483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='46c9ce7e-6a4a-4c8f-b880-2ba768ff9ac1', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Data\\\\Attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'id': 28}, page_content='i ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv\\nand WO ∈ Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:'), Document(id='cb1c26d0-86dd-42cf-92b0-c0c96a9ee076', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Data\\\\Attention.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'id': 27}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V) = Concat(head1, ...,headh)WO\\nwhere headi = Attention(QWQ\\ni , KWK\\ni , V WV\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈ Rdmodel×dk , WK\\ni ∈ Rdmodel×dk , WV\\ni ∈ Rdmodel×dv'), Document(id='dc430c3e-32ab-4e90-8fbf-7664925e60f2', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Data\\\\Attention.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'id': 13}, page_content='described in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].'), Document(id='cda3ef60-1a83-4f15-8340-9f9041bb0340', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Data\\\\Attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'id': 21}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of'), Document(id='c9fbe5d3-d5ea-460a-8012-eb9cfb165a75', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Data\\\\Attention.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'id': 60}, page_content='checkpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This'), Document(id='c3e93807-d3b4-49c1-b844-fb5f0a3fd5ec', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Data\\\\Attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'id': 17}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-'), Document(id='6fe508f6-d5ba-416c-9610-2dde3c1fa9ac', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Data\\\\Attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'id': 41}, page_content='different layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'), Document(id='ed06511d-3b46-4f34-9208-c54ade515126', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Data\\\\Attention.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'id': 38}, page_content='because it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈ Rd, such as a hidden'), Document(id='58f28a60-10d3-4a18-b7eb-1d43b02d29c1', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Data\\\\Attention.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'id': 20}, page_content='sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'), Document(id='dc91bfd0-5dfd-4cab-9846-309ab2ef6b58', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'Data\\\\Attention.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'id': 25}, page_content='extremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of')]\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Explain the multi-headed attention as compared to self attention and \n",
    "masked multi-headed attention in a detailed and easy manner.\"\"\"\n",
    "\n",
    "retrieved_documents = retriever.invoke(question)\n",
    "print(retrieved_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b4197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked Documents are :- [27, 38, 13, 20]\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.document_compressors import FlashrankRerank\n",
    "from flashrank import Ranker \n",
    "\n",
    "\n",
    "ranker = Ranker(model_name=\"ms-marco-MiniLM-L-12-v2\")\n",
    "\n",
    "compressor = FlashrankRerank(\n",
    "    score_threshold=0.2, \n",
    "    top_n=4\n",
    ")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, \n",
    "    base_retriever=retriever,\n",
    ")\n",
    "\n",
    "compressed_documents = compression_retriever.invoke(\n",
    "    question\n",
    ")\n",
    "print(\"Reranked Documents are :- {}\".format(\n",
    "    [doc.metadata[\"id\"] for doc in compressed_documents]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74f7da1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm_model_google, \n",
    "    retriever=compression_retriever, \n",
    "    chain_type=\"stuff\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4b8a74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reranked_answer = chain.invoke(\n",
    "    question\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "470d9a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's break down the differences between self-attention, multi-headed attention, and masked multi-headed attention.\n",
      "\n",
      "**1. Self-Attention:**\n",
      "\n",
      "* **Concept:** Imagine you have a sentence like \"The cat sat on the mat.\" Self-attention helps the model understand the relationships between words in this sentence. It looks at each word and tries to figure out which other words are most relevant to it. For example, \"cat\" is likely to be related to \"sat\" and \"mat.\"\n",
      "* **How it works:**\n",
      "    * **Input:** A sequence of words (like our sentence).\n",
      "    * **Process:** Each word is transformed into three vectors: a query (Q), a key (K), and a value (V). The model calculates how similar each word's query is to every other word's key. This similarity score determines how much attention each word pays to the other words.  The final output is a weighted sum of all the values, where the weights are the attention scores.\n",
      "* **Benefits:**  Self-attention allows the model to capture long-range dependencies in a sequence, meaning it can understand relationships between words that are far apart in the sentence.\n",
      "\n",
      "**2. Multi-Headed Attention:**\n",
      "\n",
      "* **Concept:**  Think of it as having multiple self-attention mechanisms working in parallel. Each \"head\" focuses on different aspects of the input sequence.\n",
      "* **How it works:**\n",
      "    * **Multiple Heads:** Instead of one set of Q, K, V vectors, you have multiple sets. Each head calculates its own attention scores and produces its own output.\n",
      "    * **Concatenation:** The outputs from all the heads are concatenated and then transformed into a final output.\n",
      "* **Benefits:**  Multi-headed attention allows the model to attend to different parts of the input sequence simultaneously, capturing a richer understanding of the relationships between words. It's like having multiple perspectives on the same information.\n",
      "\n",
      "**3. Masked Multi-Headed Attention (Used in Decoders):**\n",
      "\n",
      "* **Concept:** This is a variation of multi-headed attention used in the decoder part of a transformer model (like in language translation). It's designed to prevent the model from \"seeing\" future words during translation.\n",
      "* **How it works:**\n",
      "    * **Masking:**  The attention mechanism is masked, meaning it can only attend to words that have already been generated in the output sequence. This prevents the model from \"cheating\" by looking ahead at the target sentence.\n",
      "* **Benefits:**  Masked multi-headed attention ensures that the decoder generates the output sequence one word at a time, based only on the previously generated words, which is crucial for tasks like translation.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "* **Self-attention:**  Focuses on relationships within a single sequence.\n",
      "* **Multi-headed attention:**  Expands on self-attention by using multiple heads to capture different aspects of the input.\n",
      "* **Masked multi-headed attention:**  A specialized version of multi-headed attention used in decoders to prevent the model from looking ahead in the output sequence. \n"
     ]
    }
   ],
   "source": [
    "print(final_reranked_answer[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bcfed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
