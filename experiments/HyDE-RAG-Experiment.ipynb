{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc2251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ----> Running the Data processing pipeline for HyDE RAG...\n",
      "[INFO] ----> Please wait a while.....\n",
      "\n",
      "[INFO] ----> Creating the collection name with :- ReAct-vectorstore\n",
      "[INFO] ----> Creating the collection name with :- ReAct-VDB\n",
      "[INFO] ----> Creating the Vectorstore....\n",
      "\n",
      "[INFO] ----> Total Pages Extracted are :- 33\n",
      "[INFO] ----> Total document chunks created are :- 293\n",
      "\n",
      "[INFO] ----> Creating the Chroma DB Retriever......\n",
      "[INFO] ----> Chroma Retriever created successfully.....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.merger_retriever import MergerRetriever\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from dotenv import (\n",
    "    load_dotenv, \n",
    "    find_dotenv\n",
    ")\n",
    "import os\n",
    "\n",
    "load_dotenv(\n",
    "    find_dotenv()\n",
    ")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "def loadDataAndRetriever(data_path, **kwargs) :\n",
    "    if kwargs[\"parse_function\"] :\n",
    "        print(\"[INFO] ----> Running the Data processing pipeline for HyDE RAG...\")\n",
    "        print(\"[INFO] ----> Please wait a while.....\\n\")\n",
    "\n",
    "        collection_name = os.path.splitext(data_path)[0].split(os.sep)[1] + \"-vectorstore\"\n",
    "        print(\"[INFO] ----> Creating the collection name with :- {}\".format(collection_name))\n",
    "\n",
    "        persist_directory_name = os.path.splitext(data_path)[0].split(os.sep)[1] + \"-VDB\"\n",
    "        print(\"[INFO] ----> Creating the collection name with :- {}\".format(persist_directory_name))\n",
    "\n",
    "\n",
    "        print(\"[INFO] ----> Creating the Vectorstore....\\n\")\n",
    "        documents = PyMuPDFLoader(\n",
    "            data_path\n",
    "        ).load()\n",
    "        print(\"[INFO] ----> Total Pages Extracted are :- {}\".format(len(documents)))\n",
    "        \n",
    "\n",
    "        splitted_documents = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=512, \n",
    "            chunk_overlap=128,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        ).split_documents(documents)\n",
    "        print(\n",
    "            \"[INFO] ----> Total document chunks created are :- {}\".format(len(splitted_documents))\n",
    "        )\n",
    "\n",
    "        print(\"\\n[INFO] ----> Creating the Chroma DB Retriever......\")\n",
    "        vectorstore = Chroma.from_documents(\n",
    "            documents=splitted_documents,\n",
    "            collection_name=collection_name,\n",
    "            embedding = kwargs[\"embeddings_model\"], \n",
    "            persist_directory=persist_directory_name,\n",
    "        )\n",
    "\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_type=\"similarity\", \n",
    "            search_kwargs={\"k\" : 5},\n",
    "        )\n",
    "        print(\"[INFO] ----> Chroma Retriever created successfully.....\\n\")\n",
    "        return retriever, kwargs[\"embeddings_model\"]\n",
    "    else :\n",
    "        print(\"[INFO] ----> Change parse_function to True for execution....\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def generateAnswer(input_question, **kwargs) :\n",
    "    if kwargs[\"parse_function\"] :\n",
    "\n",
    "        user_question = input_question\n",
    "\n",
    "        verbose = -1\n",
    "\n",
    "\n",
    "        def formatDocuments(docs) :\n",
    "            return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "        hyDETemplate = \"\"\"You are an english expert mastering in creating hypothetical answers. \n",
    "        So, for the given user question generate a hypothetical answer. \n",
    "        Do not generate anything else just the answer. The question that you need to answer is: \n",
    "        Question: {user_question}\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"[INFO] ----> Creating the HyDE-Pipeline, please wait....\")\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(hyDETemplate)\n",
    "        user_question = prompt.format(\n",
    "            user_question=USER_QUESTION,\n",
    "        )\n",
    "\n",
    "        hypothetical_answer = llm_groq.invoke(user_question).content\n",
    "\n",
    "        if verbose > -1 :\n",
    "            print(\n",
    "                \"[INFO] ----> The Hypothetical Answers for the:- \\nQuestion:- {}\\nHypo-Answer:- {}\".format(user_question, hypothetical_answer)\n",
    "            )\n",
    "\n",
    "        similar_documents = retriever.invoke(\n",
    "            hypothetical_answer\n",
    "        )\n",
    "\n",
    "\n",
    "        if verbose > -1 :\n",
    "            for doc in similar_documents :\n",
    "                print(doc.page_content)\n",
    "                print(\"==\" * 100)\n",
    "        print(\"\\n[INFO] ----> Pipeline created with HyDE.....\")\n",
    "\n",
    "\n",
    "        template = \"\"\"You are an excellent assistant. \n",
    "        Answer the following question in a detailed manner based on the below provided context: \n",
    "\n",
    "        Context:- {context}\\n\n",
    "        Question:- {question}\\n\n",
    "\n",
    "        Always remember to provide a complete answer for the question asked.\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "        format_documents = formatDocuments(similar_documents)\n",
    "\n",
    "        if verbose > -1 :\n",
    "            print(\n",
    "            \"[INFO] ----> The context for the LLM is :- \\nContext :- \\n{}\".format(format_documents)\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            \"[INFO] ----> Crafting the answer for the question :- {}\\n[INFO] ----> Please Wait.....\".format(\n",
    "                USER_QUESTION\n",
    "                )\n",
    "            )\n",
    "\n",
    "        final_query = prompt.format(\n",
    "            context=format_documents, \n",
    "            question=USER_QUESTION,\n",
    "        )\n",
    "\n",
    "        hyDEResponse = llm_google.invoke(\n",
    "            final_query\n",
    "        ).content\n",
    "\n",
    "        print(\"\\n[INFO] ----> Final Answer :- {}{}\".format(\"\\n\", hyDEResponse))\n",
    "        return hyDEResponse\n",
    "    else :\n",
    "        print(\"[INFO] ----> Change parse_function to True for execution....\")\n",
    "        return None\n",
    "        \n",
    "        \n",
    "embeddings_google = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/embedding-001\"\n",
    ")\n",
    "\n",
    "llm_groq = ChatGroq(\n",
    "    model=\"gemma2-9b-it\", \n",
    "    temperature=0.3,\n",
    "    max_tokens=512,\n",
    "    model_kwargs={\"top_p\" : 0.9}\n",
    ")\n",
    "\n",
    "llm_openai = ChatOpenAI(\n",
    "    model=\"gpt-4o\", \n",
    "    max_tokens=1024, \n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "llm_google = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-001\",\n",
    "    temperature=0.2,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "retriever, embeddings = loadDataAndRetriever(\n",
    "    data_path=\"Data\\\\ReAct.pdf\",\n",
    "    embeddings_model=embeddings_google,\n",
    "    parse_function=True,\n",
    ")\n",
    "\n",
    "\n",
    "USER_QUESTION = \"What is Chain of Thought prompting and how it is related to ReAct.\"\n",
    "answer = generateAnswer(\n",
    "    input_question=USER_QUESTION, \n",
    "    parse_function=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0733ea19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ----> Creating the HyDE-Pipeline, please wait....\n",
      "\n",
      "[INFO] ----> Pipeline created with HyDE.....\n",
      "[INFO] ----> Crafting the answer for the question :- What is Chain of Thought prompting and how it is related to ReAct.\n",
      "[INFO] ----> Please Wait.....\n",
      "\n",
      "[INFO] ----> Final Answer :- \n",
      "The provided context focuses on the ReAct framework, which combines reasoning and acting to enable agents to solve tasks in an environment. While the context doesn't explicitly mention Chain of Thought (CoT) prompting, it's important to understand how these concepts relate.\n",
      "\n",
      "**Chain of Thought Prompting** is a technique used to improve the performance of large language models (LLMs) on reasoning tasks. It involves prompting the model to generate a step-by-step reasoning process, similar to how a human would think through a problem. This helps the model break down complex tasks into smaller, more manageable steps, leading to more accurate and explainable solutions.\n",
      "\n",
      "**ReAct** is a framework that combines reasoning and acting. It uses a large language model to reason about a task and then takes actions in the environment based on its reasoning. ReAct can benefit from Chain of Thought prompting in several ways:\n",
      "\n",
      "* **Improved Reasoning:** CoT prompting can help ReAct's reasoning process by guiding the model to generate more structured and logical thought chains. This can lead to more accurate and reliable actions in the environment.\n",
      "* **Explainability:** CoT prompting provides a clear and understandable explanation of the model's reasoning process. This is valuable for debugging and understanding the model's decision-making process.\n",
      "* **Generalization:** By learning to reason through problems step-by-step, ReAct can potentially generalize better to new tasks and environments.\n",
      "\n",
      "**In essence, Chain of Thought prompting can be seen as a technique that enhances the reasoning component of ReAct, leading to improved performance and explainability.** While the provided context doesn't explicitly mention CoT prompting, it's a valuable technique that could be integrated into ReAct to further enhance its capabilities. \n"
     ]
    }
   ],
   "source": [
    "USER_QUESTION = \"What is Chain of Thought prompting and how it is related to ReAct.\"\n",
    "answer = generateAnswer(\n",
    "    input_question=USER_QUESTION, \n",
    "    parse_function=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95508a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b80a726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ----> Creating the HyDE-Pipeline, please wait....\n",
      "\n",
      "[INFO] ----> Pipeline created with HyDE.....\n",
      "[INFO] ----> Crafting the answer for the question :- What is Chain of Thought prompting and how it is related to ReAct.\n",
      "[INFO] ----> Please Wait.....\n",
      "\n",
      "[INFO] ----> Final Answer :- \n",
      "The provided context focuses on the ReAct paradigm, which combines reasoning and acting with language models for task solving. While the text doesn't explicitly mention \"Chain of Thought\" prompting, it's important to understand how these concepts relate.\n",
      "\n",
      "**Chain of Thought Prompting** is a technique used to improve the reasoning abilities of language models. It involves prompting the model to generate a step-by-step reasoning process, often in the form of a series of intermediate thoughts, leading to the final answer. This helps the model break down complex problems into smaller, more manageable steps, improving its accuracy and transparency.\n",
      "\n",
      "**ReAct** builds upon the idea of reasoning by incorporating actions into the process. It prompts language models to generate both verbal reasoning traces (similar to Chain of Thought) and actions that can be executed in the real world or within a simulated environment. This allows the model to not only reason about a task but also interact with its environment to gather information or complete tasks.\n",
      "\n",
      "**Relationship between Chain of Thought and ReAct:**\n",
      "\n",
      "* **Chain of Thought** provides the foundation for the reasoning component of ReAct. ReAct leverages the ability of language models to generate step-by-step reasoning traces, similar to how Chain of Thought prompting works.\n",
      "* **ReAct extends Chain of Thought** by adding the ability to perform actions. This allows the model to not only reason about a problem but also actively engage with the environment to gather information or complete tasks.\n",
      "\n",
      "In essence, ReAct can be seen as an extension of Chain of Thought prompting, incorporating actions into the reasoning process to enable more comprehensive and interactive task solving. \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96467e97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hybrid-rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
